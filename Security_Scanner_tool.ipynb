{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iqjMBc9u61UIiuIZOufeW90IDzDpaUgB",
      "authorship_tag": "ABX9TyN4shsPBypf7ck4aeB6lfCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigyani-admin/Darsi/blob/main/Security_Scanner_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly2kwqI5QDkP",
        "outputId": "a7537558-5f0e-4f0c-a989-50901b10c66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java Development Kit (JDK)\n",
        "!apt-get install openjdk-11-jdk -qq > /dev/null\n",
        "\n",
        "# Download and install Joern\n",
        "!curl -L \"https://github.com/joernio/joern/releases/latest/download/joern-install.sh\" -o joern-install.sh\n",
        "!chmod +x joern-install.sh\n",
        "!./joern-install.sh -b /usr/local/bin\n",
        "\n",
        "print(\"Joern installed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRnXq9gVU5jF",
        "outputId": "c234c2ce-b53f-4744-8c48-543267de6d6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  6691  100  6691    0     0   7137      0 --:--:-- --:--:-- --:--:--  7137\n",
            "argument -b\n",
            "argument /usr/local/bin\n",
            "non-interactive mode, using defaults\n",
            "Installation dir: /opt/joern\n",
            "Version: \n",
            "Symbolic links in: /usr/local/bin\n",
            "/opt/joern does not exist. Creating\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1799M  100 1799M    0     0  29.4M      0  0:01:01  0:01:01 --:--:-- 29.1M\n",
            "Creating symlinks to Joern tools in /usr/local/bin\n",
            "If you are not root, please enter your password now:\n",
            "Installing default plugins\n",
            "Writing logs to: /tmp/joern-scan-log.txt\n",
            "Install complete!\n",
            "Joern installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch (ensures we have a known version)\n",
        "!pip install torch\n",
        "\n",
        "# Now, install the required PyG libraries using a more robust method\n",
        "import torch\n",
        "\n",
        "# Get the exact torch version\n",
        "TORCH_VERSION = torch.__version__\n",
        "# Construct the correct URL for the pre-built wheels\n",
        "PYG_URL = f\"https://data.pyg.org/whl/torch-{TORCH_VERSION}.html\"\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f {PYG_URL}\n",
        "# Install the main PyG library\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Install other useful libraries\n",
        "!pip install pandas scikit-learn\n",
        "\n",
        "print(\"PyTorch, PyTorch Geometric, and other libraries installed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUDxxfRuVsCK",
        "outputId": "268161bf-5f8c-4cf2-d41e-2b86224b9f45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124 torch-scatter-2.1.2+pt26cu124 torch-sparse-0.6.18+pt26cu124 torch-spline-conv-1.2.2+pt26cu124\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "PyTorch, PyTorch Geometric, and other libraries installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cloning OWASP WebGoat...\")\n",
        "!git clone https://github.com/WebGoat/WebGoat.git\n",
        "\n",
        "# Verify the contents to make sure the 'src' directory is present\n",
        "print(\"\\nVerifying clone...\")\n",
        "!ls -l WebGoat/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLqyLhG3av7k",
        "outputId": "71f7f46f-e6fe-4a8b-ab3e-7e10bbaadd5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning OWASP WebGoat...\n",
            "Cloning into 'WebGoat'...\n",
            "remote: Enumerating objects: 47593, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 47593 (delta 0), reused 0 (delta 0), pack-reused 47591 (from 2)\u001b[K\n",
            "Receiving objects: 100% (47593/47593), 102.51 MiB | 37.22 MiB/s, done.\n",
            "Resolving deltas: 100% (21557/21557), done.\n",
            "\n",
            "Verifying clone...\n",
            "total 144\n",
            "-rw-r--r-- 1 root root  4417 Jul  5 21:21 CODE_OF_CONDUCT.md\n",
            "drwxr-xr-x 6 root root  4096 Jul  5 21:21 config\n",
            "-rw-r--r-- 1 root root  8276 Jul  5 21:21 CONTRIBUTING.md\n",
            "-rw-r--r-- 1 root root  1092 Jul  5 21:21 COPYRIGHT.txt\n",
            "-rw-r--r-- 1 root root   708 Jul  5 21:21 CREATE_RELEASE.md\n",
            "-rw-r--r-- 1 root root  1391 Jul  5 21:21 Dockerfile\n",
            "-rw-r--r-- 1 root root  1396 Jul  5 21:21 Dockerfile_desktop\n",
            "drwxr-xr-x 3 root root  4096 Jul  5 21:21 docs\n",
            "-rw-r--r-- 1 root root   185 Jul  5 21:21 FAQ.md\n",
            "-rw-r--r-- 1 root root  1092 Jul  5 21:21 LICENSE.txt\n",
            "-rwxr-xr-x 1 root root   131 Jul  5 21:21 mvn-debug\n",
            "-rwxr-xr-x 1 root root 10070 Jul  5 21:21 mvnw\n",
            "-rw-r--r-- 1 root root  6609 Jul  5 21:21 mvnw.cmd\n",
            "-rw-r--r-- 1 root root 30751 Jul  5 21:21 pom.xml\n",
            "-rw-r--r-- 1 root root    56 Jul  5 21:21 PULL_REQUEST_TEMPLATE.md\n",
            "-rw-r--r-- 1 root root  1850 Jul  5 21:21 README_I18N.md\n",
            "-rw-r--r-- 1 root root  6059 Jul  5 21:21 README.md\n",
            "-rw-r--r-- 1 root root 14022 Jul  5 21:21 RELEASE_NOTES.md\n",
            "drwxr-xr-x 5 root root  4096 Jul  5 21:21 src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define paths for the OWASP WebGoat repository\n",
        "project_path = \"WebGoat/src/main/java/org/owasp/webgoat\"\n",
        "cpg_output_file = \"cpg.bin\"\n",
        "json_output_dir = \"/content/drive/MyDrive/cpg_json_output\"\n",
        "\n",
        "print(f\"Verifying project path: {project_path}\")\n",
        "if not os.path.exists(project_path):\n",
        "    print(f\"❌ ERROR: The project path '{project_path}' does not exist! Please re-run the cell that creates it.\")\n",
        "else:\n",
        "    print(\"✅ Path verified successfully.\")\n",
        "    os.makedirs(json_output_dir, exist_ok=True)\n",
        "\n",
        "    # # 1. Generate the binary CPG, capturing all output for inspection\n",
        "    # print(\"\\n--- Running joern-parse ---\")\n",
        "    # parse_command = f\"joern-parse {project_path} --language javasrc --output {cpg_output_file}\"\n",
        "    # parse_result = subprocess.run(parse_command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # # Check the output from the command for any errors\n",
        "    # if \"error\" in parse_result.stderr.lower():\n",
        "    #     print(\"❌ joern-parse may have failed. See details below:\")\n",
        "    #     print(\"--- STDOUT ---\")\n",
        "    #     print(parse_result.stdout)\n",
        "    #     print(\"--- STDERR ---\")\n",
        "    #     print(parse_result.stderr)\n",
        "    #     print(\"-\" * 20)\n",
        "    # else:\n",
        "    #     print(\"joern-parse command finished.\")\n",
        "\n",
        "    # 2. Verify that the CPG file was created AND is not empty\n",
        "    print(\"\\n--- Verifying CPG file ---\")\n",
        "    if os.path.exists(cpg_output_file) and os.path.getsize(cpg_output_file) > 100: # Check if file has meaningful content\n",
        "        print(f\"✅ CPG file '{cpg_output_file}' created successfully.\")\n",
        "        # Let's see the file size\n",
        "        !ls -lh {cpg_output_file}\n",
        "\n",
        "        # 3. If CPG is valid, run joern-export\n",
        "        print(\"\\n--- Running joern-export ---\")\n",
        "        export_command = f\"joern-export {cpg_output_file} --repr cpg --out {json_output_dir} --format json\"\n",
        "        export_result = subprocess.run(export_command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "        # Check for errors from the export command\n",
        "        if \"Error\" in export_result.stdout or \"Error\" in export_result.stderr:\n",
        "             print(\"❌ joern-export failed. Here is the output:\")\n",
        "             print(\"--- STDOUT ---\")\n",
        "             print(export_result.stdout)\n",
        "             print(\"--- STDERR ---\")\n",
        "             print(export_result.stderr)\n",
        "        else:\n",
        "             print(f\"✅ CPGs successfully exported to JSON format in: {json_output_dir}\")\n",
        "             !ls -l {json_output_dir}\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ FATAL ERROR: CPG file '{cpg_output_file}' was not created or is empty.\")\n",
        "        print(\"This confirms the joern-parse command failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77q6TYSQa7Mn",
        "outputId": "af7e0b4b-c386-4fb7-9e59-27bef44b6e37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying project path: WebGoat/src/main/java/org/owasp/webgoat\n",
            "✅ Path verified successfully.\n",
            "\n",
            "--- Verifying CPG file ---\n",
            "✅ CPG file 'cpg.bin' created successfully.\n",
            "-rw-r--r-- 1 root root 1.8M Jul  5 21:33 cpg.bin\n",
            "\n",
            "--- Running joern-export ---\n",
            "❌ joern-export failed. Here is the output:\n",
            "--- STDOUT ---\n",
            "\n",
            "--- STDERR ---\n",
            "Error: Option --format failed when given 'json'. No value found for 'json'\n",
            "Try --help for more information.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "cpg_file = \"cpg.bin\"\n",
        "output_dir = \"/content/drive/MyDrive/cpg_graphson_output\"\n",
        "\n",
        "# --- FIX: Delete the output directory first, as Joern requires it. ---\n",
        "if os.path.exists(output_dir):\n",
        "    print(f\"Found existing output directory. Deleting '{output_dir}' to allow Joern to run.\")\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "# The correct format for your Joern version is \"graphson\"\n",
        "correct_format = \"graphson\"\n",
        "\n",
        "print(f\"\\n--- Exporting CPG from '{cpg_file}' to '{correct_format}' format ---\")\n",
        "\n",
        "# This is the final, correct command\n",
        "export_command = f\"joern-export {cpg_file} --repr cpg --out {output_dir} --format {correct_format}\"\n",
        "\n",
        "print(f\"Executing command: {export_command}\")\n",
        "\n",
        "# Run the command!\n",
        "!{export_command}\n",
        "\n",
        "print(\"\\n--- Command Finished ---\")\n",
        "print(\"Checking for output files in your Google Drive...\")\n",
        "\n",
        "# Verify that the output directory now contains files\n",
        "!ls -l {output_dir}\n",
        "\n",
        "print(f\"\\n✅ If you see files listed above, it was a success!\")\n",
        "print(f\"You can now proceed with Cell 5, reading from the directory: '{output_dir}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxzCT4kmjegH",
        "outputId": "0997b64a-d3a1-4146-aa1d-a1c9bd325508"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing output directory. Deleting '/content/drive/MyDrive/cpg_graphson_output' to allow Joern to run.\n",
            "\n",
            "--- Exporting CPG from 'cpg.bin' to 'graphson' format ---\n",
            "Executing command: joern-export cpg.bin --repr cpg --out /content/drive/MyDrive/cpg_graphson_output --format graphson\n",
            "exported 46671 nodes, 260182 edges into /content/drive/MyDrive/cpg_graphson_output\n",
            "\n",
            "--- Command Finished ---\n",
            "Checking for output files in your Google Drive...\n",
            "total 20\n",
            "drwx------ 25 root root 4096 Jul  5 21:50  container\n",
            "drwx------  2 root root 4096 Jul  5 21:50 '<empty>'\n",
            "drwx------ 32 root root 4096 Jul  5 21:50  lessons\n",
            "drwx------  4 root root 4096 Jul  5 21:50  server\n",
            "drwx------ 10 root root 4096 Jul  5 21:50  webwolf\n",
            "\n",
            "✅ If you see files listed above, it was a success!\n",
            "You can now proceed with Cell 5, reading from the directory: '/content/drive/MyDrive/cpg_graphson_output'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Step 1: Set up paths and move data to the conventional directory ---\n",
        "# PyG expects raw data to be in a 'raw' subfolder of the dataset root.\n",
        "\n",
        "# This is the directory where Joern saved the JSON files, as you confirmed.\n",
        "source_raw_dir = '/content/drive/MyDrive/cpg_graphson_output' # Despite the name, this has our JSONs\n",
        "\n",
        "# This will be the root directory for our processed PyG dataset.\n",
        "dataset_root_path = '/content/drive/MyDrive/cpg_dataset_webgoat'\n",
        "dest_raw_dir = os.path.join(dataset_root_path, 'raw')\n",
        "\n",
        "# If the source exists, move it to the destination expected by PyTorch Geometric.\n",
        "if os.path.exists(source_raw_dir):\n",
        "    print(f\"Moving raw data from '{source_raw_dir}' to '{dest_raw_dir}'...\")\n",
        "    # If the destination already exists from a previous run, remove it first.\n",
        "    if os.path.exists(dest_raw_dir):\n",
        "        shutil.rmtree(dest_raw_dir)\n",
        "    shutil.move(source_raw_dir, dest_raw_dir)\n",
        "    print(\"✅ Done.\")\n",
        "else:\n",
        "    print(f\"⚠️ Raw data source not found, assuming it's already in '{dest_raw_dir}'.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Define the Custom Dataset Class ---\n",
        "\n",
        "class CpgFuncGraphDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        # Load the processed data from disk\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        # CORRECTED: Find all files ending in .json inside all subdirectories.\n",
        "        file_list = []\n",
        "        for dirpath, _, filenames in os.walk(self.raw_dir):\n",
        "            for filename in filenames:\n",
        "                if filename.endswith('.json'):\n",
        "                    # Create a path relative to the raw directory root\n",
        "                    rel_path = os.path.relpath(os.path.join(dirpath, filename), self.raw_dir)\n",
        "                    file_list.append(rel_path)\n",
        "        return file_list\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass # Data is already locally generated\n",
        "\n",
        "    def process(self):\n",
        "        # First, build a global vocabulary of all node labels across the entire dataset.\n",
        "        # This is critical for creating consistent one-hot encodings for the model.\n",
        "        print(\"Building node label vocabulary...\")\n",
        "        all_node_labels = set()\n",
        "        skipped_files_scan = 0\n",
        "        for raw_path in tqdm(self.raw_paths, desc=\"Scanning files\"):\n",
        "            with open(raw_path, 'r') as f:\n",
        "                try:\n",
        "                    cpg_json_list = json.load(f)\n",
        "                    # Handle both single graph and list of graphs per file\n",
        "                    if not isinstance(cpg_json_list, list):\n",
        "                        cpg_json_list = [cpg_json_list]\n",
        "\n",
        "                    for graph_data in cpg_json_list:\n",
        "                      if 'nodes' in graph_data:\n",
        "                          for node in graph_data['nodes']:\n",
        "                              all_node_labels.add(node.get('_label', 'UNKNOWN'))\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Skipping malformed JSON file during scan: {raw_path}\")\n",
        "                    skipped_files_scan += 1\n",
        "\n",
        "        label_map = {label: i for i, label in enumerate(sorted(list(all_node_labels)))}\n",
        "        num_node_types = len(label_map)\n",
        "        print(f\"Found {num_node_types} unique node types.\")\n",
        "        if skipped_files_scan > 0:\n",
        "             print(f\"Skipped {skipped_files_scan} malformed JSON files during scanning.\")\n",
        "\n",
        "\n",
        "        # Now, process each file into a PyG Data object.\n",
        "        data_list = []\n",
        "        skipped_files_process = 0\n",
        "        skipped_graphs = 0\n",
        "        for raw_path in tqdm(self.raw_paths, desc=\"Processing CPGs\"):\n",
        "            with open(raw_path, 'r') as f:\n",
        "                try:\n",
        "                    cpg_json_list = json.load(f)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Skipping malformed JSON file during processing: {raw_path}\")\n",
        "                    skipped_files_process += 1\n",
        "                    continue # Skip malformed files\n",
        "\n",
        "            # Ensure we can handle files that contain a single graph or a list of them\n",
        "            if not isinstance(cpg_json_list, list):\n",
        "                cpg_json_list = [cpg_json_list]\n",
        "\n",
        "            for cpg_data in cpg_json_list:\n",
        "                if not cpg_data.get('nodes') or not cpg_data.get('edges'):\n",
        "                    skipped_graphs += 1\n",
        "                    continue # Skip graphs with no nodes or edges\n",
        "\n",
        "                # 1. Create Node Features (x) using the global label map\n",
        "                node_indices = [label_map[node.get('_label', 'UNKNOWN')] for node in cpg_data['nodes']]\n",
        "                node_features = torch.nn.functional.one_hot(\n",
        "                    torch.tensor(node_indices),\n",
        "                    num_classes=num_node_types\n",
        "                ).float()\n",
        "\n",
        "                # 2. Create Edge Index\n",
        "                node_id_map = {node['_id']: i for i, node in enumerate(cpg_data['nodes'])}\n",
        "\n",
        "                # Add a check to ensure node_id_map is not empty\n",
        "                if not node_id_map:\n",
        "                    skipped_graphs += 1\n",
        "                    continue # Skip graphs if node_id_map is empty\n",
        "\n",
        "                edge_list = [[node_id_map.get(edge['_outV']), node_id_map.get(edge['_inV'])]\n",
        "                             for edge in cpg_data['edges']\n",
        "                             if node_id_map.get(edge['_outV']) is not None and node_id_map.get(edge['_inV']) is not None]\n",
        "\n",
        "                if not edge_list:\n",
        "                    skipped_graphs += 1\n",
        "                    continue # Skip graphs with no valid edges after mapping\n",
        "\n",
        "                edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "                # 3. Create Label (y)\n",
        "                # Heuristic for WebGoat: label as vulnerable (1) if from a \"lesson\" folder, secure (0) otherwise\n",
        "                is_vulnerable = 1 if 'lessons' in raw_path.lower() else 0\n",
        "                label = torch.tensor([is_vulnerable], dtype=torch.long)\n",
        "\n",
        "                data = Data(x=node_features, edge_index=edge_index, y=label)\n",
        "                data_list.append(data)\n",
        "\n",
        "        if skipped_files_process > 0:\n",
        "             print(f\"Skipped {skipped_files_process} malformed JSON files during processing.\")\n",
        "        if skipped_graphs > 0:\n",
        "             print(f\"Skipped {skipped_graphs} graphs with no nodes, edges, or valid node mappings.\")\n",
        "\n",
        "\n",
        "        # Check if data_list is empty before collating\n",
        "        if not data_list:\n",
        "            raise ValueError(\"No valid graphs were processed. data_list is empty.\")\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "# --- Step 3: Instantiate the dataset ---\n",
        "dataset = CpgFuncGraphDataset(root=dataset_root_path)\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Number of node features: {dataset.num_features}\")\n",
        "print(f\"Number of classes: {dataset.num_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "Nn2OYe9bmMXV",
        "outputId": "335852c1-5257-49e2-92e4-06df7b229c6b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Raw data source not found, assuming it's already in '/content/drive/MyDrive/cpg_dataset_webgoat/raw'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building node label vocabulary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning files: 100%|██████████| 1707/1707 [00:09<00:00, 189.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 unique node types.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing CPGs: 100%|██████████| 1707/1707 [00:07<00:00, 243.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 1707 graphs with no nodes, edges, or valid node mappings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No valid graphs were processed. data_list is empty.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-2434021386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# --- Step 3: Instantiate the dataset ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCpgFuncGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_root_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nDataset loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2434021386.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCpgFuncGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Load the processed data from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mforce_reload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     ) -> None:\n\u001b[0;32m---> 81\u001b[0;31m         super().__init__(root, transform, pre_transform, pre_filter, log,\n\u001b[0m\u001b[1;32m     82\u001b[0m                          force_reload)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pre_transform.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2434021386.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Check if data_list is empty before collating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid graphs were processed. data_list is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No valid graphs were processed. data_list is empty."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Let's inspect the first JSON file we find to understand its structure.\n",
        "raw_dir = '/content/drive/MyDrive/cpg_dataset_webgoat/raw'\n",
        "first_file_path = None\n",
        "\n",
        "# Find the first .json file in the directory tree\n",
        "for dirpath, _, filenames in os.walk(raw_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.json'):\n",
        "            first_file_path = os.path.join(dirpath, filename)\n",
        "            break\n",
        "    if first_file_path:\n",
        "        break\n",
        "\n",
        "if first_file_path:\n",
        "    print(f\"--- Inspecting file: {first_file_path} ---\")\n",
        "    with open(first_file_path, 'r') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            # Check if the file contains a list or a single dictionary\n",
        "            if isinstance(data, list):\n",
        "                print(\"\\nFile contains a LIST of objects.\")\n",
        "                if data:\n",
        "                    print(\"Keys of the FIRST object in the list:\", list(data[0].keys()))\n",
        "            elif isinstance(data, dict):\n",
        "                print(\"\\nFile contains a single DICTIONARY object.\")\n",
        "                print(\"Top-level keys of the dictionary:\", list(data.keys()))\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Could not parse the file as JSON.\")\n",
        "else:\n",
        "    print(\"Could not find any JSON files to inspect in the 'raw' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4cWIDgboGIy",
        "outputId": "0dd53e32-a129-4d5a-9a33-b2491ade0812"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inspecting file: /content/drive/MyDrive/cpg_dataset_webgoat/raw/container/AjaxAuthenticationEntryPoint.java/_init_.json ---\n",
            "\n",
            "File contains a single DICTIONARY object.\n",
            "Top-level keys of the dictionary: ['@type', '@value']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Let's inspect the first JSON file again, but go one level deeper.\n",
        "raw_dir = '/content/drive/MyDrive/cpg_dataset_webgoat/raw'\n",
        "first_file_path = None\n",
        "\n",
        "# Find the first .json file\n",
        "for dirpath, _, filenames in os.walk(raw_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.json'):\n",
        "            first_file_path = os.path.join(dirpath, filename)\n",
        "            break\n",
        "    if first_file_path:\n",
        "        break\n",
        "\n",
        "if first_file_path:\n",
        "    print(f\"--- Deeply inspecting file: {first_file_path} ---\")\n",
        "    with open(first_file_path, 'r') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            if '@value' in data:\n",
        "                print(\"✅ Found the '@value' key.\")\n",
        "                value_content = data['@value']\n",
        "\n",
        "                # Now let's analyze the content of the '@value' key\n",
        "                if isinstance(value_content, list):\n",
        "                    print(f\"✅ The '@value' key contains a LIST with {len(value_content)} item(s).\")\n",
        "                    if value_content:\n",
        "                        first_item = value_content[0]\n",
        "                        print(f\"✅ The first item in the list is a: {type(first_item)}\")\n",
        "                        if isinstance(first_item, dict):\n",
        "                            print(\"✅ Keys of the first item in the list:\", list(first_item.keys()))\n",
        "                        else:\n",
        "                            # If it's not a dictionary, just print some of it\n",
        "                             print(\"Content of the first item:\", str(first_item)[:500])\n",
        "                    else:\n",
        "                        print(\"❗️ The list inside '@value' is EMPTY.\")\n",
        "\n",
        "                elif isinstance(value_content, dict):\n",
        "                    print(\"✅ The '@value' key contains a DICTIONARY.\")\n",
        "                    print(\"Keys of the dictionary inside '@value':\", list(value_content.keys()))\n",
        "                else:\n",
        "                    print(f\"❗️ The '@value' key contains an unexpected type: {type(value_content)}\")\n",
        "            else:\n",
        "                print(\"❗️ The '@value' key was NOT found in the top level of the JSON.\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"❌ Could not parse the file as JSON.\")\n",
        "else:\n",
        "    print(\"❌ Could not find any JSON files to inspect in the 'raw' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzqB6l4foeJM",
        "outputId": "9b07cdc9-7adf-4121-f3fd-62bae3f38fe0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Deeply inspecting file: /content/drive/MyDrive/cpg_dataset_webgoat/raw/container/AjaxAuthenticationEntryPoint.java/_init_.json ---\n",
            "✅ Found the '@value' key.\n",
            "✅ The '@value' key contains a DICTIONARY.\n",
            "Keys of the dictionary inside '@value': ['edges', 'vertices']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Step 1: Prepare Directories (No changes here) ---\n",
        "source_raw_dir = '/content/drive/MyDrive/cpg_graphson_output'\n",
        "dataset_root_path = '/content/drive/MyDrive/cpg_dataset_webgoat'\n",
        "dest_raw_dir = os.path.join(dataset_root_path, 'raw')\n",
        "\n",
        "if os.path.exists(source_raw_dir):\n",
        "    print(f\"Moving raw data from '{source_raw_dir}' to '{dest_raw_dir}'...\")\n",
        "    if os.path.exists(dest_raw_dir):\n",
        "        shutil.rmtree(dest_raw_dir)\n",
        "    shutil.move(source_raw_dir, dest_raw_dir)\n",
        "    print(\"✅ Done.\")\n",
        "else:\n",
        "    print(f\"⚠️ Raw data source not found, assuming it's already in '{dest_raw_dir}'.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Define the Custom Dataset Class (Corrected) ---\n",
        "\n",
        "class CpgFuncGraphDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        file_list = []\n",
        "        for dirpath, _, filenames in os.walk(self.raw_dir):\n",
        "            for filename in filenames:\n",
        "                if filename.endswith('.json'):\n",
        "                    rel_path = os.path.relpath(os.path.join(dirpath, filename), self.raw_dir)\n",
        "                    file_list.append(rel_path)\n",
        "        return file_list\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        # Build a global vocabulary of all node labels\n",
        "        print(\"Building node label vocabulary...\")\n",
        "        all_node_labels = set()\n",
        "        for raw_path in tqdm(self.raw_paths, desc=\"Scanning files\"):\n",
        "            full_path = os.path.join(self.raw_dir, raw_path)\n",
        "            with open(full_path, 'r') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    graph_data = data.get('@value', {})\n",
        "\n",
        "                    # ✅ FIXED: Use 'vertices' instead of 'nodes'\n",
        "                    if 'vertices' in graph_data:\n",
        "                        for vertex in graph_data['vertices']:\n",
        "                            all_node_labels.add(vertex.get('_label', 'UNKNOWN'))\n",
        "                except (json.JSONDecodeError, AttributeError):\n",
        "                    print(f\"Skipping malformed file: {raw_path}\")\n",
        "\n",
        "        label_map = {label: i for i, label in enumerate(sorted(list(all_node_labels)))}\n",
        "        num_node_types = len(label_map)\n",
        "        print(f\"Found {num_node_types} unique node types.\")\n",
        "\n",
        "        # Process each file into a PyG Data object\n",
        "        data_list = []\n",
        "        for raw_path in tqdm(self.raw_paths, desc=\"Processing CPGs\"):\n",
        "            full_path = os.path.join(self.raw_dir, raw_path)\n",
        "            with open(full_path, 'r') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    cpg_data = data.get('@value', {})\n",
        "                except (json.JSONDecodeError, AttributeError):\n",
        "                    continue\n",
        "\n",
        "                # ✅ FIXED: Use 'vertices' instead of 'nodes'\n",
        "                if not cpg_data.get('vertices') or not cpg_data.get('edges'):\n",
        "                    continue\n",
        "\n",
        "                # 1. Create Node Features (x)\n",
        "                # ✅ FIXED: Use 'vertices'\n",
        "                vertices = cpg_data['vertices']\n",
        "                node_indices = [label_map.get(node.get('_label', 'UNKNOWN')) for node in vertices]\n",
        "                if None in node_indices: continue\n",
        "                node_features = torch.nn.functional.one_hot(torch.tensor(node_indices), num_classes=num_node_types).float()\n",
        "\n",
        "                # 2. Create Edge Index\n",
        "                # ✅ FIXED: Use 'vertices'\n",
        "                node_id_map = {node['_id']: i for i, node in enumerate(vertices)}\n",
        "                edge_list = [[node_id_map.get(edge['_outV']), node_id_map.get(edge['_inV'])]\n",
        "                             for edge in cpg_data['edges']\n",
        "                             if node_id_map.get(edge['_outV']) is not None and node_id_map.get(edge['_inV']) is not None]\n",
        "\n",
        "                if not edge_list: continue\n",
        "                edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "                # 3. Create Label (y)\n",
        "                is_vulnerable = 1 if 'lessons' in raw_path.lower() else 0\n",
        "                label = torch.tensor([is_vulnerable], dtype=torch.long)\n",
        "\n",
        "                data_obj = Data(x=node_features, edge_index=edge_index, y=label)\n",
        "                data_list.append(data_obj)\n",
        "\n",
        "        if not data_list:\n",
        "            raise ValueError(\"No valid graphs were processed. The data_list is empty.\")\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "# --- Step 3: Instantiate the dataset ---\n",
        "dataset = CpgFuncGraphDataset(root=dataset_root_path)\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Number of node features: {dataset.num_features}\")\n",
        "print(f\"Number of classes: {dataset.num_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "IvhsuoTlqvOB",
        "outputId": "bc4d9e23-b2e8-463d-890e-212bafe4e34d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Raw data source not found, assuming it's already in '/content/drive/MyDrive/cpg_dataset_webgoat/raw'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building node label vocabulary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning files: 100%|██████████| 1707/1707 [00:08<00:00, 190.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 unique node types.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing CPGs:   0%|          | 0/1707 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-2286970736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# --- Step 3: Instantiate the dataset ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCpgFuncGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_root_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nDataset loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-2286970736.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCpgFuncGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mforce_reload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     ) -> None:\n\u001b[0;32m---> 81\u001b[0;31m         super().__init__(root, transform, pre_transform, pre_filter, log,\n\u001b[0m\u001b[1;32m     82\u001b[0m                          force_reload)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pre_transform.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-2286970736.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# 2. Create Edge Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# ✅ FIXED: Use 'vertices'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mnode_id_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 edge_list = [[node_id_map.get(edge['_outV']), node_id_map.get(edge['_inV'])]\n\u001b[1;32m     95\u001b[0m                              \u001b[0;32mfor\u001b[0m \u001b[0medge\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcpg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edges'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-2286970736.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# 2. Create Edge Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# ✅ FIXED: Use 'vertices'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mnode_id_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 edge_list = [[node_id_map.get(edge['_outV']), node_id_map.get(edge['_inV'])]\n\u001b[1;32m     95\u001b[0m                              \u001b[0;32mfor\u001b[0m \u001b[0medge\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcpg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edges'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Let's find the keys of a single vertex to identify the correct ID key.\n",
        "raw_dir = '/content/drive/MyDrive/cpg_dataset_webgoat/raw'\n",
        "file_to_inspect = None\n",
        "\n",
        "# Find the first .json file to inspect\n",
        "for dirpath, _, filenames in os.walk(raw_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.json'):\n",
        "            file_to_inspect = os.path.join(dirpath, filename)\n",
        "            break\n",
        "    if file_to_inspect:\n",
        "        break\n",
        "\n",
        "if file_to_inspect:\n",
        "    print(f\"--- Inspecting vertex keys in file: {file_to_inspect} ---\")\n",
        "    with open(file_to_inspect, 'r') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            graph_data = data.get('@value', {})\n",
        "\n",
        "            if 'vertices' in graph_data and graph_data['vertices']:\n",
        "                # Get the very first vertex from the list\n",
        "                first_vertex = graph_data['vertices'][0]\n",
        "                print(\"\\n✅ Success! Found the first vertex.\")\n",
        "                print(\"✅ The keys for a single vertex are:\", list(first_vertex.keys()))\n",
        "            else:\n",
        "                print(\"❗️ Could not find a 'vertices' list with content in this file.\")\n",
        "\n",
        "        except (json.JSONDecodeError, AttributeError, IndexError):\n",
        "            print(\"❌ Could not parse file or find a vertex.\")\n",
        "else:\n",
        "    print(\"❌ Could not find any JSON files to inspect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4F3g4FIrKRz",
        "outputId": "0448c488-a22a-47b5-c140-531158054803"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inspecting vertex keys in file: /content/drive/MyDrive/cpg_dataset_webgoat/raw/container/AjaxAuthenticationEntryPoint.java/_init_.json ---\n",
            "\n",
            "✅ Success! Found the first vertex.\n",
            "✅ The keys for a single vertex are: ['@type', 'id', 'label', 'properties']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Step 1: Prepare Directories ---\n",
        "source_raw_dir = '/content/drive/MyDrive/cpg_graphson_output'\n",
        "dataset_root_path = '/content/drive/MyDrive/cpg_dataset_webgoat'\n",
        "dest_raw_dir = os.path.join(dataset_root_path, 'raw')\n",
        "\n",
        "if os.path.exists(source_raw_dir):\n",
        "    print(f\"Moving raw data from '{source_raw_dir}' to '{dest_raw_dir}'...\")\n",
        "    if os.path.exists(dest_raw_dir):\n",
        "        shutil.rmtree(dest_raw_dir)\n",
        "    shutil.move(source_raw_dir, dest_raw_dir)\n",
        "    print(\"✅ Done.\")\n",
        "else:\n",
        "    print(f\"⚠️ Raw data source not found, assuming it's already in '{dest_raw_dir}'.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Define the Custom Dataset Class (Final Version) ---\n",
        "class CpgFuncGraphDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        # ✅ FINAL FIX: Add weights_only=False due to a PyTorch security update.\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        file_list = []\n",
        "        for dirpath, _, filenames in os.walk(self.raw_dir):\n",
        "            for filename in filenames:\n",
        "                if filename.endswith('.json'):\n",
        "                    rel_path = os.path.relpath(os.path.join(dirpath, filename), self.raw_dir)\n",
        "                    file_list.append(rel_path)\n",
        "        return file_list\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        print(\"Building node label vocabulary...\")\n",
        "        all_node_labels = set()\n",
        "        for raw_path in tqdm(self.raw_paths, desc=\"Scanning files\"):\n",
        "            full_path = os.path.join(self.raw_dir, raw_path)\n",
        "            with open(full_path, 'r') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    graph_data = data.get('@value', {})\n",
        "                    if 'vertices' in graph_data:\n",
        "                        for vertex in graph_data['vertices']:\n",
        "                            all_node_labels.add(vertex.get('label', 'UNKNOWN'))\n",
        "                except (json.JSONDecodeError, AttributeError):\n",
        "                    print(f\"Skipping malformed file: {raw_path}\")\n",
        "\n",
        "        label_map = {label: i for i, label in enumerate(sorted(list(all_node_labels)))}\n",
        "        num_node_types = len(label_map)\n",
        "        print(f\"Found {num_node_types} unique node types.\")\n",
        "\n",
        "        data_list = []\n",
        "        for raw_path in tqdm(self.raw_paths, desc=\"Processing CPGs\"):\n",
        "            full_path = os.path.join(self.raw_dir, raw_path)\n",
        "            with open(full_path, 'r') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    cpg_data = data.get('@value', {})\n",
        "                except (json.JSONDecodeError, AttributeError):\n",
        "                    continue\n",
        "\n",
        "                if not cpg_data.get('vertices') or not cpg_data.get('edges'):\n",
        "                    continue\n",
        "\n",
        "                vertices = cpg_data['vertices']\n",
        "                node_indices = [label_map.get(node.get('label', 'UNKNOWN')) for node in vertices]\n",
        "                if None in node_indices: continue\n",
        "                node_features = torch.nn.functional.one_hot(torch.tensor(node_indices), num_classes=num_node_types).float()\n",
        "\n",
        "                node_id_map = {node['id']['@value']: i for i, node in enumerate(vertices)}\n",
        "                edge_list = [[node_id_map.get(edge['outV']['@value']), node_id_map.get(edge['inV']['@value'])]\n",
        "                             for edge in cpg_data['edges']\n",
        "                             if node_id_map.get(edge['outV']['@value']) is not None and node_id_map.get(edge['inV']['@value']) is not None]\n",
        "\n",
        "                if not edge_list: continue\n",
        "                edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "                is_vulnerable = 1 if 'lessons' in raw_path.lower() else 0\n",
        "                label = torch.tensor([is_vulnerable], dtype=torch.long)\n",
        "                data_obj = Data(x=node_features, edge_index=edge_index, y=label)\n",
        "                data_list.append(data_obj)\n",
        "\n",
        "        if not data_list:\n",
        "            raise ValueError(\"No valid graphs were processed. The data_list is empty.\")\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "# --- Step 3: Instantiate the dataset ---\n",
        "\n",
        "# To ensure a clean run, we'll remove the old 'processed' directory one time.\n",
        "processed_dir = os.path.join(dataset_root_path, 'processed')\n",
        "if os.path.exists(processed_dir):\n",
        "    print(f\"Removing old processed data directory to ensure a clean run: {processed_dir}\")\n",
        "    shutil.rmtree(processed_dir)\n",
        "\n",
        "dataset = CpgFuncGraphDataset(root=dataset_root_path)\n",
        "\n",
        "print(f\"\\n✅ Dataset loaded successfully!\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Number of node features: {dataset.num_features}\")\n",
        "print(f\"Number of classes: {dataset.num_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8pHHzukre_z",
        "outputId": "51d9290c-bfac-4457-fdb5-30cad9a23c9e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Raw data source not found, assuming it's already in '/content/drive/MyDrive/cpg_dataset_webgoat/raw'.\n",
            "Removing old processed data directory to ensure a clean run: /content/drive/MyDrive/cpg_dataset_webgoat/processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building node label vocabulary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning files: 100%|██████████| 1707/1707 [00:08<00:00, 194.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 21 unique node types.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing CPGs: 100%|██████████| 1707/1707 [00:07<00:00, 216.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Dataset loaded successfully!\n",
            "Number of graphs: 1707\n",
            "Number of node features: 21\n",
            "Number of classes: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Let's inspect the structure of the 'id' field itself.\n",
        "raw_dir = '/content/drive/MyDrive/cpg_dataset_webgoat/raw'\n",
        "file_to_inspect = None\n",
        "\n",
        "# Find the first .json file to inspect\n",
        "for dirpath, _, filenames in os.walk(raw_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.json'):\n",
        "            file_to_inspect = os.path.join(dirpath, filename)\n",
        "            break\n",
        "    if file_to_inspect:\n",
        "        break\n",
        "\n",
        "if file_to_inspect:\n",
        "    print(f\"--- Inspecting the 'id' key in file: {file_to_inspect} ---\")\n",
        "    with open(file_to_inspect, 'r') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            graph_data = data.get('@value', {})\n",
        "\n",
        "            if 'vertices' in graph_data and graph_data['vertices']:\n",
        "                first_vertex = graph_data['vertices'][0]\n",
        "\n",
        "                if 'id' in first_vertex:\n",
        "                    id_value = first_vertex['id']\n",
        "                    print(f\"✅ Found the 'id' key. Its type is: {type(id_value)}\")\n",
        "\n",
        "                    if isinstance(id_value, dict):\n",
        "                        print(\"✅ The node 'id' is a dictionary. Its keys are:\", list(id_value.keys()))\n",
        "                    else:\n",
        "                        print(\"The 'id' is a simple value:\", id_value)\n",
        "                else:\n",
        "                    print(\"❗️ The 'id' key was not found in the first vertex.\")\n",
        "            else:\n",
        "                print(\"❗️ Could not find a 'vertices' list with content in this file.\")\n",
        "\n",
        "        except (json.JSONDecodeError, AttributeError, IndexError):\n",
        "            print(\"❌ Could not parse file or find a vertex.\")\n",
        "else:\n",
        "    print(\"❌ Could not find any JSON files to inspect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REd0LXp8sINk",
        "outputId": "efee35df-cf4f-46fe-bdb6-2b99fbca53eb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inspecting the 'id' key in file: /content/drive/MyDrive/cpg_dataset_webgoat/raw/container/AjaxAuthenticationEntryPoint.java/_init_.json ---\n",
            "✅ Found the 'id' key. Its type is: <class 'dict'>\n",
            "✅ The node 'id' is a dictionary. Its keys are: ['@type', '@value']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# --- Define Paths ---\n",
        "# This points to the dataset folder we created in the last step\n",
        "dataset_root_path = '/content/drive/MyDrive/cpg_dataset_webgoat'\n",
        "# ✅ We will save the trained model inside this same folder for organization.\n",
        "model_save_path = os.path.join(dataset_root_path, 'gnn_vuln_detector.pt')\n",
        "\n",
        "\n",
        "# 1. Define the GCN Model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index).relu()\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "# 2. Prepare DataLoaders and Split Data\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:int(len(dataset) * 0.8)]\n",
        "test_dataset = dataset[int(len(dataset) * 0.8):]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 3. Initialize Model, Optimizer, and Loss Function\n",
        "model = GCN(\n",
        "    in_channels=dataset.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    out_channels=dataset.num_classes\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. Define Training and Testing Functions\n",
        "def train():\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# 5. Run the Training Loop\n",
        "for epoch in range(1, 51):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "# Save the trained model to the path defined at the top\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"\\n✅ Model training complete. Final model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEQVyXnWx_9Q",
        "outputId": "96a9adb3-5a89-49c8-8c3b-97ff9f56a238"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 1365\n",
            "Number of test graphs: 342\n",
            "Epoch: 010, Train Acc: 0.7245, Test Acc: 0.7310\n",
            "Epoch: 020, Train Acc: 0.7846, Test Acc: 0.7778\n",
            "Epoch: 030, Train Acc: 0.7802, Test Acc: 0.7632\n",
            "Epoch: 040, Train Acc: 0.7861, Test Acc: 0.7690\n",
            "Epoch: 050, Train Acc: 0.7780, Test Acc: 0.7427\n",
            "\n",
            "✅ Model training complete. Final model saved to /content/drive/MyDrive/cpg_dataset_webgoat/gnn_vuln_detector.pt\n"
          ]
        }
      ]
    }
  ]
}